import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import h5py
import os
import pywt
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
from torchdiffeq import odeint
from contextlib import contextmanager
from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, classification_report, confusion_matrix
# ================ DATA LOADING ================

class WiMANSDataset(Dataset):
    """Dataset for WiMANS WiFi CSI data"""

    def __init__(self, data_path, split='train', transform_wavelet=True, wavelet='db4', level=3, device=None):
        """
        Args:
            data_path: Path to processed data
            split: 'train', 'val', or 'test'
            transform_wavelet: Whether to apply wavelet transform
            wavelet: Wavelet type to use
            level: Decomposition level for wavelet transform
            device: Device to store data (None for CPU)
        """
        self.data_path = data_path
        self.split = split
        self.transform_wavelet = transform_wavelet
        self.wavelet = wavelet
        self.level = level
        self.device = device

        # Load data
        h5_file = h5py.File(os.path.join(data_path, f'{split}_data.h5'), 'r')
        self.amplitude = h5_file['X_amplitude'][:]
        self.phase = h5_file['X_phase'][:]
        self.activity_labels = h5_file['y_activity'][:]
        self.location_labels = h5_file['y_location'][:]
        h5_file.close()

        # Load activity and location mappings
        self.activity_mapping = np.load(os.path.join(data_path, 'activity_mapping.npy'), allow_pickle=True).item()
        self.location_mapping = np.load(os.path.join(data_path, 'location_mapping.npy'), allow_pickle=True).item()

        # Get class counts
        self.num_activities = len(self.activity_mapping)
        self.num_locations = len(self.location_mapping)

        # Cache for wavelet features
        self.wavelet_cache = {}

        print(f"Loaded {split} dataset with {len(self.amplitude)} samples")
        print(f"Amplitude shape: {self.amplitude.shape}")
        print(f"Phase shape: {self.phase.shape}")
        print(f"Activities: {list(self.activity_mapping.keys())}")

    def __len__(self):
        return len(self.amplitude)

    def __getitem__(self, idx):
        # Get CSI sample
        amplitude = self.amplitude[idx].astype(np.float32)
        phase = self.phase[idx].astype(np.float32)

        # Get labels
        activity = self.activity_labels[idx]
        location = self.location_labels[idx]

        # Convert location to one-hot encoding
        location_onehot = np.zeros(self.num_locations, dtype=np.float32)
        location_onehot[location] = 1.0

        if self.transform_wavelet:
            # Apply wavelet transform, with caching for efficiency
            if idx in self.wavelet_cache:
                wavelet_features = self.wavelet_cache[idx]
            else:
                wavelet_features = self._apply_wavelet_transform(amplitude, phase)
                # Only cache if we have reasonable memory (not too many samples)
                if len(self.wavelet_cache) < 1000:
                    self.wavelet_cache[idx] = wavelet_features

            # Determine random grid coordinates for adaptive grid module
            # In a real implementation, these might be extracted from the data
            grid_coords = np.random.randint(0, 10, size=(10, 2)).astype(np.int32)

            return {
                'amplitude': torch.tensor(amplitude, dtype=torch.float32),
                'phase': torch.tensor(phase, dtype=torch.float32),
                'wavelet': torch.tensor(wavelet_features, dtype=torch.float32),
                'grid_coords': torch.tensor(grid_coords, dtype=torch.long),
                'activity_label': torch.tensor(activity, dtype=torch.long),
                'location_label': torch.tensor(location_onehot, dtype=torch.float32)
            }
        else:
            # Determine random grid coordinates for adaptive grid module
            grid_coords = np.random.randint(0, 10, size=(10, 2)).astype(np.int32)

            return {
                'amplitude': torch.tensor(amplitude, dtype=torch.float32),
                'phase': torch.tensor(phase, dtype=torch.float32),
                'grid_coords': torch.tensor(grid_coords, dtype=torch.long),
                'activity_label': torch.tensor(activity, dtype=torch.long),
                'location_label': torch.tensor(location_onehot, dtype=torch.float32)
            }

    def _apply_wavelet_transform(self, amplitude, phase):
        """Apply wavelet transform to CSI data with adaptive reshaping"""
        # Select a subset of time points for computational efficiency
        time_points = np.linspace(0, amplitude.shape[0]-1, 10, dtype=int)

        # Reshape for wavelet transform - collapse Rx/Tx dimensions
        wavelet_features_list = []

        for t in time_points:
            # Get data for this time step
            amp_t = amplitude[t].reshape(-1, amplitude.shape[-1])  # (Rx*Tx, subcarriers)
            phase_t = phase[t].reshape(-1, phase.shape[-1])

            features_t = []

            # Process each antenna pair
            for i in range(amp_t.shape[0]):
                # Apply wavelet transform to amplitude
                coeffs_amp = pywt.wavedec(amp_t[i], self.wavelet, level=self.level)
                # Apply wavelet transform to phase
                coeffs_phase = pywt.wavedec(phase_t[i], self.wavelet, level=self.level)

                # Concatenate coefficients
                features = np.concatenate([np.concatenate(coeffs_amp), np.concatenate(coeffs_phase)])
                features_t.append(features)

            # Combine features for this time step
            wavelet_features_list.append(np.concatenate(features_t))

        # Combine all time steps
        wavelet_features = np.array(wavelet_features_list)

        # Adaptively reshape based on actual size
        feature_size = wavelet_features.size

        # Option 1: Resize to the closest 2D shape
        rows = 10  # Keep the first dimension as 10 (time points)
        cols = feature_size // rows

        # If there's a remainder, we need to pad
        if feature_size % rows != 0:
            cols += 1
            # Create a padded array
            padded = np.zeros((rows, cols))
            flat = wavelet_features.flatten()
            padded.flat[:flat.size] = flat
            return padded
        else:
            # Perfect fit, just reshape
            return wavelet_features.reshape(rows, cols)

    def close(self):
        """Clean up any resources"""
        # Clear cache to free memory
        self.wavelet_cache.clear()


# ================ LIQUID NEURAL NETWORK COMPONENTS ================

class LiquidODEFunction(nn.Module):
    """ODE function defining the dynamics of the liquid neural network"""
    def __init__(self, dim, hidden_multiplier=4):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, dim * hidden_multiplier),
            nn.Tanh(),
            nn.Linear(dim * hidden_multiplier, dim)
        )

    def forward(self, t, x):
        return self.net(x)

class LiquidAttention(nn.Module):
    """Attention mechanism with continuous-time dynamics"""
    def __init__(self, embed_dim, num_heads, integration_time=0.5):
        super().__init__()
        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.layer_norm1 = nn.LayerNorm(embed_dim)
        self.layer_norm2 = nn.LayerNorm(embed_dim)
        self.ode_func = LiquidODEFunction(embed_dim)
        self.integration_time = integration_time

    def forward(self, x):
        # Apply layer normalization before attention
        x_norm = self.layer_norm1(x)

        # Standard attention mechanism
        attn_output, _ = self.mha(x_norm, x_norm, x_norm)

        # Initial state with residual connection
        z0 = x + attn_output

        # Define integration times
        t = torch.tensor([0, self.integration_time]).float().to(x.device)

        # Integrate ODE with numerical stability settings
        z_t = odeint(self.ode_func, z0, t, method='dopri5', rtol=1e-3, atol=1e-4)

        # Return final state after normalization
        return self.layer_norm2(z_t[-1])

class LiquidFeedForward(nn.Module):
    """Feed-forward network with continuous-time dynamics"""
    def __init__(self, embed_dim, ff_dim, integration_time=0.5):
        super().__init__()
        # Prevent division by zero by ensuring minimum multiplier of 2
        hidden_multiplier = max(2, ff_dim//embed_dim) if embed_dim > 0 else 4
        self.ode_func = LiquidODEFunction(embed_dim, hidden_multiplier=hidden_multiplier)
        self.layer_norm = nn.LayerNorm(embed_dim)
        self.integration_time = integration_time

    def forward(self, x):
        # Apply layer normalization
        x_norm = self.layer_norm(x)

        # Define integration times
        t = torch.tensor([0, self.integration_time]).float().to(x.device)

        # Integrate ODE with residual connection
        z_t = odeint(self.ode_func, x + 0.1 * x_norm, t, method='dopri5', rtol=1e-3, atol=1e-4)

        # Return final state
        return z_t[-1]

class LiquidTransformerEncoder(nn.Module):
    """Transformer encoder with liquid neural network dynamics"""
    def __init__(self, embed_dim, num_heads, ff_dim, num_layers, integration_time=0.5):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.ModuleList([
                LiquidAttention(embed_dim, num_heads, integration_time),
                LiquidFeedForward(embed_dim, ff_dim, integration_time)
            ])
            for _ in range(num_layers)
        ])

    def forward(self, x):
        for attn, ff in self.layers:
            # Apply liquid attention
            x = attn(x)

            # Apply liquid feed-forward
            x = ff(x)

        return x

class CrossLiquidAttention(nn.Module):
    """Cross-attention between different data streams with liquid dynamics"""
    def __init__(self, embed_dim, num_heads, integration_time=0.5):
        super().__init__()
        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.layer_norm1 = nn.LayerNorm(embed_dim)
        self.layer_norm2 = nn.LayerNorm(embed_dim)
        self.ode_func = LiquidODEFunction(embed_dim)
        self.integration_time = integration_time

    def forward(self, x, context):
        # Apply layer normalization
        x_norm = self.layer_norm1(x)
        context_norm = self.layer_norm2(context)

        # Cross-attention mechanism
        attn_output, _ = self.mha(x_norm, context_norm, context_norm)

        # Initial state with residual connection
        z0 = x + attn_output

        # Define integration times
        t = torch.tensor([0, self.integration_time]).float().to(x.device)

        # Integrate ODE with numerical stability settings
        z_t = odeint(self.ode_func, z0, t, method='dopri5', rtol=1e-3, atol=1e-4)

        # Return final state
        return z_t[-1]

class AdaptiveGridModule(nn.Module):
    """Adaptive spatial grid module using simple feature aggregation"""
    def __init__(self, input_dim, hidden_dim, grid_size=10):
        super().__init__()
        self.grid_size = grid_size

        # Grid cell feature extractor
        self.cell_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # Simple feature aggregation
        self.aggregation = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # Grid density predictor
        self.density_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )

    def forward(self, features, grid_coords):
        batch_size = features.shape[0]

        # Initialize grid
        grid_features = torch.zeros(batch_size, self.grid_size, self.grid_size,
                                  features.shape[-1], device=features.device)

        # Assign features to grid cells based on coordinates
        for b in range(batch_size):
            for i, (x, y) in enumerate(grid_coords[b]):
                x, y = min(max(int(x), 0), self.grid_size-1), min(max(int(y), 0), self.grid_size-1)
                grid_features[b, x, y] = features[b, i % features.shape[1]]

        # Encode cell features
        grid_features = grid_features.reshape(batch_size, -1, features.shape[-1])
        cell_features = self.cell_encoder(grid_features)

        # Simple feature aggregation
        aggregated_features = torch.mean(cell_features, dim=1, keepdim=True).expand_as(cell_features)
        combined = torch.cat([cell_features, aggregated_features], dim=-1)
        updated_features = cell_features + self.aggregation(combined)

        # Predict cell densities
        densities = self.density_predictor(updated_features)

        # Reshape back to grid
        updated_grid = updated_features.reshape(batch_size, self.grid_size, self.grid_size, -1)
        densities_grid = densities.reshape(batch_size, self.grid_size, self.grid_size, -1)

        return updated_grid, densities_grid

class BayesianHead(nn.Module):
    """Bayesian classification head with uncertainty estimation"""
    def __init__(self, input_dim, output_dim, num_samples=10):
        super().__init__()
        self.num_samples = num_samples

        # Mean network
        self.mean_net = nn.Linear(input_dim, output_dim)

        # Log variance network
        self.logvar_net = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        # Compute mean and log variance
        mean = self.mean_net(x)
        logvar = self.logvar_net(x)

        if self.training:
            # Sample using reparameterization trick
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            samples = mean + eps * std
            return samples, mean, logvar
        else:
            # During inference, perform Monte Carlo sampling
            samples = []
            for _ in range(self.num_samples):
                std = torch.exp(0.5 * logvar)
                eps = torch.randn_like(std)
                samples.append(mean + eps * std)

            # Return mean prediction and uncertainty
            samples = torch.stack(samples, dim=0)
            prediction = torch.mean(samples, dim=0)
            uncertainty = torch.var(samples, dim=0)

            return prediction, uncertainty


# ================ FULL LNN WIFI SENSING MODEL ================

class LiquidWiFiSensingModel(nn.Module):
    """WiFi sensing model with Liquid Neural Network-based transformers"""

    def __init__(self, config):
        super().__init__()

        # Configuration
        self.config = config
        self.integration_time = config.get('integration_time', 0.5)

        # Embedding dimensions
        self.amplitude_dim = config.get('amplitude_dim', 256)
        self.phase_dim = config.get('phase_dim', 256)
        self.wavelet_dim = config.get('wavelet_dim', 256)

        # Get input shapes from config
        self.amplitude_shape = config['amplitude_shape']  # [time, rx, tx, subcarriers]
        self.phase_shape = config['phase_shape']  # [time, rx, tx, subcarriers]
        self.wavelet_shape = config['wavelet_shape']  # [rows, cols]

        # 1. Signal preprocessing

        # Amplitude embedding - using 3D convolutional layers
        self.amplitude_conv = nn.Sequential(
            nn.Conv3d(1, 16, kernel_size=3, padding=1),
            nn.BatchNorm3d(16),
            nn.ReLU(),
            nn.MaxPool3d(kernel_size=(1, 1, 2)),
            nn.Conv3d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm3d(32),
            nn.ReLU(),
            nn.MaxPool3d(kernel_size=2),
        )

        # Calculate flattened size for amplitude
        try:
            with torch.no_grad():
                # Create a dummy input with the correct shape for Conv3D: [batch, channel, D, H, W]
                # We'll combine time and rx for D dimension to fit Conv3D requirements
                amp_dummy = torch.zeros(1, 1,
                                       self.amplitude_shape[0],  # time as depth
                                       self.amplitude_shape[1] * self.amplitude_shape[2],  # rx*tx as height
                                       self.amplitude_shape[3])  # subcarriers as width
                amp_conv_out = self.amplitude_conv(amp_dummy)
                self.amp_flatten_size = amp_conv_out.numel()
        except Exception as e:
            print(f"Warning: Error calculating amplitude flattened size: {e}")
            # Fallback to a reasonable size if shape inference fails
            self.amp_flatten_size = 1024

        self.amplitude_flatten = nn.Flatten()
        self.amplitude_embed = nn.Linear(self.amp_flatten_size, self.amplitude_dim)

        # Phase embedding - using 3D convolutional layers
        self.phase_conv = nn.Sequential(
            nn.Conv3d(1, 16, kernel_size=3, padding=1),
            nn.BatchNorm3d(16),
            nn.ReLU(),
            nn.MaxPool3d(kernel_size=(1, 1, 2)),
            nn.Conv3d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm3d(32),
            nn.ReLU(),
            nn.MaxPool3d(kernel_size=2),
        )

        # Calculate flattened size for phase
        try:
            with torch.no_grad():
                # Same approach as amplitude
                phase_dummy = torch.zeros(1, 1,
                                         self.phase_shape[0],  # time as depth
                                         self.phase_shape[1] * self.phase_shape[2],  # rx*tx as height
                                         self.phase_shape[3])  # subcarriers as width
                phase_conv_out = self.phase_conv(phase_dummy)
                self.phase_flatten_size = phase_conv_out.numel()
        except Exception as e:
            print(f"Warning: Error calculating phase flattened size: {e}")
            # Fallback to a reasonable size if shape inference fails
            self.phase_flatten_size = 1024

        self.phase_flatten = nn.Flatten()
        self.phase_embed = nn.Linear(self.phase_flatten_size, self.phase_dim)

        # Wavelet processing with 2D CNN
        self.wavelet_conv = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))  # Adaptive pooling to fixed size
        )

        # Calculate flattened size for wavelet
        try:
            with torch.no_grad():
                wavelet_dummy = torch.zeros(1, 1, *self.wavelet_shape)
                wavelet_conv_out = self.wavelet_conv(wavelet_dummy)
                self.wavelet_flatten_size = wavelet_conv_out.numel()
        except Exception as e:
            print(f"Warning: Error calculating wavelet flattened size: {e}")
            # Fallback to a reasonable size if shape inference fails
            self.wavelet_flatten_size = 64

        self.wavelet_flatten = nn.Flatten()
        self.wavelet_embed = nn.Linear(self.wavelet_flatten_size, self.wavelet_dim)

        # 2. LNN-based Transformer encoders for each stream
        self.amplitude_transformer = LiquidTransformerEncoder(
            self.amplitude_dim, config['num_heads'], config['ff_dim'],
            config['num_layers'], self.integration_time
        )

        self.phase_transformer = LiquidTransformerEncoder(
            self.phase_dim, config['num_heads'], config['ff_dim'],
            config['num_layers'], self.integration_time
        )

        self.wavelet_transformer = LiquidTransformerEncoder(
            self.wavelet_dim, config['num_heads'], config['ff_dim'],
            config['num_layers'], self.integration_time
        )

        # 3. Cross-attention modules with liquid dynamics
        self.amp_phase_cross = CrossLiquidAttention(
            self.amplitude_dim, config['num_heads'], self.integration_time
        )

        self.amp_wavelet_cross = CrossLiquidAttention(
            self.amplitude_dim, config['num_heads'], self.integration_time
        )

        self.phase_wavelet_cross = CrossLiquidAttention(
            self.phase_dim, config['num_heads'], self.integration_time
        )

        # 4. Feature fusion
        self.fusion_layer = nn.Sequential(
            nn.Linear(self.amplitude_dim + self.phase_dim + self.wavelet_dim,
                     config['fusion_dim']),
            nn.Tanh(),  # Using Tanh instead of ReLU for smoother gradients
            nn.Dropout(0.2)
        )

        # 5. Adaptive grid module
        self.adaptive_grid = AdaptiveGridModule(
            config['fusion_dim'], config['grid_dim'], config['grid_size']
        )

        # 6. Bayesian prediction heads
        self.location_head = BayesianHead(
            config['grid_dim'], config['num_locations']
        )

        self.activity_head = BayesianHead(
            config['grid_dim'] + config['fusion_dim'], config['num_activities']
        )

        # 7. Auxiliary heads (optional)
        self.use_auxiliary_heads = config.get('use_auxiliary_heads', False)
        if self.use_auxiliary_heads:
            self.count_head = nn.Linear(config['fusion_dim'], 1)
            self.posture_head = nn.Linear(config['fusion_dim'], config.get('num_postures', 5))

        print(f"LNN-based Model initialized successfully")
        print(f"Amplitude flatten size: {self.amp_flatten_size}")
        print(f"Phase flatten size: {self.phase_flatten_size}")
        print(f"Wavelet flatten size: {self.wavelet_flatten_size}")

    def forward(self, amplitude, phase, wavelet=None, grid_coords=None):
        batch_size = amplitude.shape[0]

        # 1. Process amplitude stream
        # Reshape from [batch, time, rx, tx, subcarriers] -> [batch, 1, time, rx*tx, subcarriers]
        amplitude_reshaped = amplitude.reshape(
            batch_size,
            1,  # channels
            self.amplitude_shape[0],  # time (depth)
            self.amplitude_shape[1] * self.amplitude_shape[2],  # rx*tx (height)
            self.amplitude_shape[3]  # subcarriers (width)
        )

        amp_conv = self.amplitude_conv(amplitude_reshaped)
        amp_flat = self.amplitude_flatten(amp_conv)
        amp_embedded = self.amplitude_embed(amp_flat)

        # Prepare for transformer (batch, seq_len, dim)
        amp_features = amp_embedded.unsqueeze(1)
        amp_features = self.amplitude_transformer(amp_features)

        # 2. Process phase stream
        phase_reshaped = phase.reshape(
            batch_size,
            1,  # channels
            self.phase_shape[0],  # time (depth)
            self.phase_shape[1] * self.phase_shape[2],  # rx*tx (height)
            self.phase_shape[3]  # subcarriers (width)
        )

        phase_conv = self.phase_conv(phase_reshaped)
        phase_flat = self.phase_flatten(phase_conv)
        phase_embedded = self.phase_embed(phase_flat)

        # Prepare for transformer (batch, seq_len, dim)
        phase_features = phase_embedded.unsqueeze(1)
        phase_features = self.phase_transformer(phase_features)

        # 3. Process wavelet stream (if available)
        if wavelet is not None:
            # Add channel dimension for 2D convolution [batch, rows, cols] -> [batch, 1, rows, cols]
            wavelet = wavelet.unsqueeze(1)
            wavelet_conv = self.wavelet_conv(wavelet)
            wavelet_flat = self.wavelet_flatten(wavelet_conv)
            wavelet_embedded = self.wavelet_embed(wavelet_flat)

            # Prepare for transformer (batch, seq_len, dim)
            wavelet_features = wavelet_embedded.unsqueeze(1)
            wavelet_features = self.wavelet_transformer(wavelet_features)
        else:
            # Create dummy wavelet features
            wavelet_features = torch.zeros(batch_size, 1, self.wavelet_dim, device=amplitude.device)

        # 4. Cross-attention between streams using liquid dynamics
        amp_with_phase = self.amp_phase_cross(amp_features, phase_features)
        amp_with_wavelet = self.amp_wavelet_cross(amp_features, wavelet_features)
        phase_with_wavelet = self.phase_wavelet_cross(phase_features, wavelet_features)

        # 5. Feature fusion
        combined_features = torch.cat([
            amp_with_phase.squeeze(1),
            phase_with_wavelet.squeeze(1),
            wavelet_features.squeeze(1)
        ], dim=-1)

        fused_features = self.fusion_layer(combined_features)

        # 6. Adaptive grid processing (if grid coordinates are provided)
        if grid_coords is not None:
            # Expand fused features for grid processing
            expanded_features = fused_features.unsqueeze(1).expand(-1, 10, -1)
            grid_features, densities = self.adaptive_grid(expanded_features, grid_coords)
        else:
            # Create dummy grid features
            grid_features = torch.zeros(batch_size, self.config['grid_size'], self.config['grid_size'],
                                      self.config['grid_dim'], device=amplitude.device)
            densities = torch.zeros(batch_size, self.config['grid_size'], self.config['grid_size'],
                                  1, device=amplitude.device)

        # Flatten grid features for prediction
        flat_grid = grid_features.reshape(batch_size, -1, grid_features.shape[-1])
        global_grid = torch.mean(flat_grid, dim=1)

        # 7. Predictions

        # Location prediction
        if self.training:
            location_pred, loc_mean, loc_logvar = self.location_head(global_grid)
        else:
            location_pred, location_uncertainty = self.location_head(global_grid)

        # Activity prediction
        activity_input = torch.cat([global_grid, fused_features], dim=-1)

        if self.training:
            activity_pred, act_mean, act_logvar = self.activity_head(activity_input)
        else:
            activity_pred, activity_uncertainty = self.activity_head(activity_input)

        # Auxiliary predictions (if enabled)
        if self.use_auxiliary_heads and self.training:
            count_pred = self.count_head(fused_features)
            posture_pred = self.posture_head(fused_features)
        else:
            count_pred = None
            posture_pred = None

        # Return predictions based on training mode
        if self.training:
            return {
                'location': location_pred,
                'activity': activity_pred,
                'count': count_pred,
                'posture': posture_pred,
                'densities': densities,
                'loc_mean': loc_mean,
                'loc_logvar': loc_logvar,
                'act_mean': act_mean,
                'act_logvar': act_logvar
            }
        else:
            return {
                'location': location_pred,
                'activity': activity_pred,
                'densities': densities,
                'location_uncertainty': location_uncertainty,
                'activity_uncertainty': activity_uncertainty
            }


# ================ LOSS FUNCTION ================

class MultiTaskLoss(nn.Module):
    """Loss function combining multiple tasks with uncertainty weighting"""

    def __init__(self, task_weights=None):
        super().__init__()
        self.task_weights = task_weights or {
            'location': 1.0,
            'activity': 1.0,
            'count': 0.5,
            'posture': 0.5,
            'kl': 0.1
        }

    def forward(self, outputs, targets):
        losses = {}

        # Location loss (binary cross entropy for multi-label)
        if 'location' in outputs and 'location_label' in targets:
            losses['location'] = F.binary_cross_entropy_with_logits(
                outputs['location'], targets['location_label']
            )

        # Activity loss (cross entropy)
        if 'activity' in outputs and 'activity_label' in targets:
            losses['activity'] = F.cross_entropy(
                outputs['activity'], targets['activity_label']
            )

        # Count loss (MSE) - optional
        if 'count' in outputs and outputs['count'] is not None and 'count_label' in targets:
            losses['count'] = F.mse_loss(
                outputs['count'], targets['count_label']
            )
        else:
            losses['count'] = torch.tensor(0.0, device=outputs['activity'].device)

        # Posture loss (cross entropy) - optional
        if 'posture' in outputs and outputs['posture'] is not None and 'posture_label' in targets:
            losses['posture'] = F.cross_entropy(
                outputs['posture'], targets['posture_label']
            )
        else:
            losses['posture'] = torch.tensor(0.0, device=outputs['activity'].device)

        # KL divergence for Bayesian heads
        if 'loc_mean' in outputs and 'loc_logvar' in outputs:
            kl_loc = -0.5 * torch.sum(1 + outputs['loc_logvar'] -
                                     outputs['loc_mean'].pow(2) -
                                     outputs['loc_logvar'].exp())
            kl_act = -0.5 * torch.sum(1 + outputs['act_logvar'] -
                                     outputs['act_mean'].pow(2) -
                                     outputs['act_logvar'].exp())
            losses['kl'] = (kl_loc + kl_act) / targets['activity_label'].shape[0]  # Normalize by batch size
        else:
            losses['kl'] = torch.tensor(0.0, device=outputs['activity'].device)

        # Combine losses with weights
        total_loss = sum(self.task_weights[k] * v for k, v in losses.items())
        losses['total'] = total_loss

        return total_loss, {k: v.item() for k, v in losses.items()}


# ================ TRAINING FUNCTIONS ================

# Context manager for ODE solver errors
@contextmanager
def handle_ode_errors():
    try:
        yield
    except Exception as e:
        if "dopri5" in str(e) or "NaN" in str(e) or "nan" in str(e):
            print(f"ODE solver error: {e}")
            print("Trying to continue with gradient clipping...")
            # Let the outer code handle it
            raise RuntimeError("ODE solver failed, try reducing learning rate or integration time")
        else:
            raise  # Re-raise other exceptions

def train_model(model, train_loader, val_loader, config):
    """Train the WiFi sensing model with LNN-based transformers"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    model = model.to(device)

    # Loss function and optimizer
    criterion = MultiTaskLoss(config.get('task_weights'))
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )

    # Learning rate scheduler
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )

    best_val_loss = float('inf')
    patience_counter = 0
    early_stop_patience = config.get('early_stop_patience', 15)

    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_activity_acc': [],
        'val_activity_acc': []
    }

    for epoch in range(config['epochs']):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['epochs']} [Train]")
        for batch in progress_bar:
            # Move data to device
            amplitude = batch['amplitude'].to(device)
            phase = batch['phase'].to(device)
            activity_label = batch['activity_label'].to(device)
            location_label = batch['location_label'].to(device)

            # Create auxiliary inputs if available
            wavelet = batch.get('wavelet', None)
            if wavelet is not None:
                wavelet = wavelet.to(device)

            grid_coords = batch.get('grid_coords', None)
            if grid_coords is not None:
                grid_coords = grid_coords.to(device)

            # Forward pass with ODE error handling
            try:
                with handle_ode_errors():
                    outputs = model(amplitude, phase, wavelet, grid_coords)

                    # Prepare targets
                    targets = {
                        'activity_label': activity_label,
                        'location_label': location_label
                    }

                    # Calculate loss
                    loss, loss_components = criterion(outputs, targets)

                    # Backward pass and optimize
                    optimizer.zero_grad()
                    loss.backward()

                    # Gradient clipping - important for ODE stability
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])

                    optimizer.step()
            except RuntimeError as e:
                if "ODE solver failed" in str(e):
                    print("Skipping this batch due to ODE solver issues")
                    continue
                else:
                    raise

            # Track metrics
            train_loss += loss.item()

            # Calculate accuracy for activity classification
            _, predicted = torch.max(outputs['activity'].data, 1)
            train_total += activity_label.size(0)
            train_correct += (predicted == activity_label).sum().item()

            # Update progress bar
            progress_bar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'act_acc': f"{100 * train_correct / train_total:.2f}%"
            })

        train_loss /= len(train_loader)
        train_acc = 100 * train_correct / train_total

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            progress_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{config['epochs']} [Val]")
            for batch in progress_bar:
                # Move data to device
                amplitude = batch['amplitude'].to(device)
                phase = batch['phase'].to(device)
                activity_label = batch['activity_label'].to(device)
                location_label = batch['location_label'].to(device)

                # Create auxiliary inputs if available
                wavelet = batch.get('wavelet', None)
                if wavelet is not None:
                    wavelet = wavelet.to(device)

                grid_coords = batch.get('grid_coords', None)
                if grid_coords is not None:
                    grid_coords = grid_coords.to(device)

                # Forward pass
                try:
                    with handle_ode_errors():
                        outputs = model(amplitude, phase, wavelet, grid_coords)

                        # Prepare targets
                        targets = {
                            'activity_label': activity_label,
                            'location_label': location_label
                        }

                        # Calculate loss
                        loss, loss_components = criterion(outputs, targets)
                except RuntimeError as e:
                    if "ODE solver failed" in str(e):
                        print("Skipping this batch during validation due to ODE solver issues")
                        continue
                    else:
                        raise

                # Track metrics
                val_loss += loss.item()

                # Calculate accuracy for activity classification
                _, predicted = torch.max(outputs['activity'].data, 1)
                val_total += activity_label.size(0)
                val_correct += (predicted == activity_label).sum().item()

                # Update progress bar
                progress_bar.set_postfix({
                    'loss': f"{loss.item():.4f}",
                    'act_acc': f"{100 * val_correct / val_total:.2f}%"
                })

        val_loss /= len(val_loader)
        val_acc = 100 * val_correct / val_total

        # Update learning rate scheduler
        scheduler.step(val_loss)

        # Save history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_activity_acc'].append(train_acc)
        history['val_activity_acc'].append(val_acc)

        # Print progress
        print(f"Epoch {epoch+1}/{config['epochs']}:")
        print(f"  Train Loss: {train_loss:.4f}, Train Activity Acc: {train_acc:.2f}%")
        print(f"  Val Loss: {val_loss:.4f}, Val Activity Acc: {val_acc:.2f}%")

        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_acc': val_acc,
            }, config['save_path'])
            print(f"  Saved best model with val_loss: {val_loss:.4f}")
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping
        if patience_counter >= early_stop_patience:
            print(f"Early stopping after {epoch+1} epochs")
            break

        print("-" * 50)

    # Plot training history
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Loss Curves')

    plt.subplot(1, 2, 2)
    plt.plot(history['train_activity_acc'], label='Train Accuracy')
    plt.plot(history['val_activity_acc'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.title('Activity Classification Accuracy')

    plt.tight_layout()
    plt.savefig(os.path.join(os.path.dirname(config['save_path']), 'training_history.png'))
    plt.show()

    # Return the model and training history
    return model, history


def enhanced_evaluate_model(model, test_loader, config):
    """Enhanced evaluation with ROC curves, standard deviation metrics, and classification report"""
    import torch
    import numpy as np
    import matplotlib.pyplot as plt
    from tqdm import tqdm
    from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, classification_report, confusion_matrix

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()

    test_loss = 0.0
    test_correct = 0
    test_total = 0

    # For storing predictions and true labels
    all_activities = []
    all_predictions = []
    all_probabilities = []  # For ROC curves
    all_uncertainties = []  # For uncertainty analysis

    # For standard deviation across batches
    batch_accuracies = []

    # Create MultiTaskLoss
    criterion = MultiTaskLoss(config.get('task_weights'))

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluating"):
            # Move data to device
            amplitude = batch['amplitude'].to(device)
            phase = batch['phase'].to(device)
            activity_label = batch['activity_label'].to(device)
            location_label = batch['location_label'].to(device)

            # Create auxiliary inputs if available
            wavelet = batch.get('wavelet', None)
            if wavelet is not None:
                wavelet = wavelet.to(device)

            grid_coords = batch.get('grid_coords', None)
            if grid_coords is not None:
                grid_coords = grid_coords.to(device)

            # Forward pass
            try:
                with handle_ode_errors():
                    outputs = model(amplitude, phase, wavelet, grid_coords)

                    # Prepare targets
                    targets = {
                        'activity_label': activity_label,
                        'location_label': location_label
                    }

                    # Calculate loss
                    loss, _ = criterion(outputs, targets)
            except RuntimeError as e:
                if "ODE solver failed" in str(e):
                    print("Skipping this batch during evaluation due to ODE solver issues")
                    continue
                else:
                    raise

            test_loss += loss.item()

            # Get activity predictions and uncertainty
            activity_pred = outputs['activity']
            activity_uncertainty = outputs.get('activity_uncertainty')

            # Get softmax probabilities for ROC curves
            probabilities = torch.nn.functional.softmax(activity_pred, dim=1)

            # Calculate accuracy
            _, predicted = torch.max(activity_pred, 1)
            batch_size = activity_label.size(0)
            test_total += batch_size
            correct = (predicted == activity_label).sum().item()
            test_correct += correct

            # Calculate batch accuracy for standard deviation
            batch_accuracies.append(correct / batch_size * 100)

            # Store predictions and true labels
            all_activities.extend(activity_label.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())
            all_probabilities.append(probabilities.cpu().numpy())

            if activity_uncertainty is not None:
                all_uncertainties.extend(activity_uncertainty.mean(dim=1).cpu().numpy())

    # Concatenate all probabilities
    all_probabilities = np.vstack(all_probabilities)

    # Convert lists to numpy arrays
    all_activities = np.array(all_activities)
    all_predictions = np.array(all_predictions)

    # Calculate overall metrics
    test_loss /= len(test_loader)
    test_acc = 100 * test_correct / test_total
    acc_std = np.std(batch_accuracies)

    # Print basic metrics
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Activity Accuracy: {test_acc:.2f}% ± {acc_std:.2f}%")

    # Calculate precision, recall, and F1-score
    precision, recall, f1, support = precision_recall_fscore_support(
        all_activities, all_predictions, average='weighted'
    )

    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # Print full classification report
    class_names = list(test_loader.dataset.activity_mapping.keys())
    cr = classification_report(all_activities, all_predictions,
                              target_names=class_names,
                              digits=4)
    print("\nClassification Report:")
    print(cr)

    # Save classification report to file
    with open(os.path.join(os.path.dirname(config['save_path']), 'classification_report.txt'), 'w') as f:
        f.write(f"Test Accuracy: {test_acc:.2f}% ± {acc_std:.2f}%\n\n")
        f.write(cr)

    # Confusion matrix
    num_classes = config['num_activities']
    cm = confusion_matrix(all_activities, all_predictions)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    # Plot confusion matrix
    plt.figure(figsize=(12, 10))
    plt.imshow(cm_normalized, interpolation='nearest', cmap='Blues')
    plt.title('Normalized Confusion Matrix')
    plt.colorbar()

    # Add class labels
    tick_marks = np.arange(num_classes)
    plt.xticks(tick_marks, class_names, rotation=45, ha='right')
    plt.yticks(tick_marks, class_names)

    # Add values to the plot
    thresh = cm_normalized.max() / 2.
    for i in range(cm_normalized.shape[0]):
        for j in range(cm_normalized.shape[1]):
            plt.text(j, i, f"{cm_normalized[i, j]:.2f}",
                     ha="center", va="center",
                     color="white" if cm_normalized[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

    # Save confusion matrix
    plt.savefig(os.path.join(os.path.dirname(config['save_path']), 'confusion_matrix.png'))
    plt.show()

    # ROC curves
    plt.figure(figsize=(12, 10))

    # Calculate ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    for i in range(num_classes):
        fpr[i], tpr[i], _ = roc_curve(
            (all_activities == i).astype(int),
            all_probabilities[:, i]
        )
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(
        np.eye(num_classes)[all_activities].ravel(),
        all_probabilities.ravel()
    )
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

    # Plot all ROC curves
    plt.plot(
        fpr["micro"],
        tpr["micro"],
        label=f'micro-average ROC (AUC = {roc_auc["micro"]:.2f})',
        color='deeppink',
        linestyle=':',
        linewidth=4
    )

    # Plot individual class ROC curves
    for i, color in zip(range(num_classes), plt.cm.rainbow(np.linspace(0, 1, num_classes))):
        plt.plot(
            fpr[i],
            tpr[i],
            color=color,
            linewidth=2,
            label=f'ROC {class_names[i]} (AUC = {roc_auc[i]:.2f})'
        )

    plt.plot([0, 1], [0, 1], 'k--', linewidth=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curves')
    plt.legend(loc="lower right")

    # Save ROC curves
    plt.savefig(os.path.join(os.path.dirname(config['save_path']), 'roc_curves.png'))
    plt.show()

    # If we have uncertainties, plot them
    if all_uncertainties:
        plt.figure(figsize=(12, 10))
        correct = np.array(all_predictions) == np.array(all_activities)

        # Plot uncertainty distributions
        plt.subplot(2, 2, 1)
        plt.hist(np.array(all_uncertainties)[correct], alpha=0.5, label='Correct')
        plt.hist(np.array(all_uncertainties)[~correct], alpha=0.5, label='Incorrect')
        plt.xlabel('Uncertainty')
        plt.ylabel('Count')
        plt.legend()
        plt.title('Uncertainty Distribution')

        # Plot accuracy vs uncertainty
        plt.subplot(2, 2, 2)
        uncertainty_bins = np.linspace(min(all_uncertainties), max(all_uncertainties), 10)
        accuracies = []
        bin_centers = []
        bin_stds = []

        for i in range(len(uncertainty_bins)-1):
            mask = (np.array(all_uncertainties) >= uncertainty_bins[i]) & (np.array(all_uncertainties) < uncertainty_bins[i+1])
            if mask.sum() > 0:
                acc = np.mean(correct[mask])
                acc_std = np.std(correct[mask]) / np.sqrt(mask.sum())  # Standard error
                accuracies.append(acc)
                bin_stds.append(acc_std)
                bin_centers.append((uncertainty_bins[i] + uncertainty_bins[i+1]) / 2)

        plt.errorbar(bin_centers, accuracies, yerr=bin_stds, fmt='o-')
        plt.xlabel('Uncertainty')
        plt.ylabel('Accuracy')
        plt.title('Accuracy vs Uncertainty (with Standard Error)')

        # Plot uncertainty by class
        plt.subplot(2, 2, 3)
        class_uncertainties = [[] for _ in range(num_classes)]
        for i, (true, unc) in enumerate(zip(all_activities, all_uncertainties)):
            class_uncertainties[true].append(unc)

        box_data = [np.array(u) for u in class_uncertainties if len(u) > 0]
        plt.boxplot(box_data)
        plt.xticks(range(1, len(box_data)+1), [class_names[i] for i, u in enumerate(class_uncertainties) if len(u) > 0],
                  rotation=45, ha='right')
        plt.ylabel('Uncertainty')
        plt.title('Uncertainty Distribution by Class')

        # Plot correct vs incorrect uncertainty by class
        plt.subplot(2, 2, 4)
        correct_by_class = [[] for _ in range(num_classes)]
        incorrect_by_class = [[] for _ in range(num_classes)]

        for true, pred, unc in zip(all_activities, all_predictions, all_uncertainties):
            if true == pred:
                correct_by_class[true].append(unc)
            else:
                incorrect_by_class[true].append(unc)

        # Bar plot showing mean uncertainty for correct and incorrect predictions by class
        x = np.arange(num_classes)
        width = 0.35

        correct_means = [np.mean(c) if len(c) > 0 else 0 for c in correct_by_class]
        correct_stds = [np.std(c) / np.sqrt(max(1, len(c))) if len(c) > 0 else 0 for c in correct_by_class]

        incorrect_means = [np.mean(c) if len(c) > 0 else 0 for c in incorrect_by_class]
        incorrect_stds = [np.std(c) / np.sqrt(max(1, len(c))) if len(c) > 0 else 0 for c in incorrect_by_class]

        plt.bar(x - width/2, correct_means, width, label='Correct', yerr=correct_stds)
        plt.bar(x + width/2, incorrect_means, width, label='Incorrect', yerr=incorrect_stds)

        plt.xticks(x, class_names, rotation=45, ha='right')
        plt.ylabel('Mean Uncertainty')
        plt.title('Uncertainty for Correct vs. Incorrect Predictions by Class')
        plt.legend()

        plt.tight_layout()
        plt.savefig(os.path.join(os.path.dirname(config['save_path']), 'uncertainty_analysis.png'))
        plt.show()

    return {
        'test_loss': test_loss,
        'test_acc': test_acc,
        'test_acc_std': acc_std,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc["micro"],
        'class_roc_auc': {class_names[i]: roc_auc[i] for i in range(num_classes)},
        'confusion_matrix': cm,
    }


def main(data_path, output_path):
    """Main function to train and evaluate the model"""
    # Create output directory
    os.makedirs(output_path, exist_ok=True)

    # Load the processed data
    print("Loading datasets...")
    train_dataset = WiMANSDataset(data_path, split='train', transform_wavelet=True)
    val_dataset = WiMANSDataset(data_path, split='val', transform_wavelet=True)
    test_dataset = WiMANSDataset(data_path, split='test', transform_wavelet=True)

    # Define configuration
    config = {
        # Data shapes from the dataset
        'amplitude_shape': (100, 3, 3, 30),  # [time, rx, tx, subcarriers]
        'phase_shape': (100, 3, 3, 30),      # [time, rx, tx, subcarriers]
        # 'amplitude_shape': (500, 3, 3, 30),  # Update from 100 to 500
        # 'phase_shape': (500, 3, 3, 30),      # Update from 100 to 500
        'wavelet_shape': (10, 100),          # Assuming wavelet transform output shape

        # Model hyperparameters
        'amplitude_dim': 128,
        'phase_dim': 128,
        'wavelet_dim': 128,
        'num_heads': 4,
        'ff_dim': 256,
        'num_layers': 2,
        'fusion_dim': 256,
        'grid_dim': 128,
        'grid_size': 10,
        'num_activities': train_dataset.num_activities,
        'num_locations': train_dataset.num_locations,
        'use_auxiliary_heads': False,  # Set to True if you have posture/count labels

        # Integration time parameter for liquid neural networks
        'integration_time': 0.5,       # ODE integration time for liquid neural networks (reduced for stability)

        # Training hyperparameters
        'batch_size': 32,
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'epochs': 30,
        'max_grad_norm': 1.0,
        'early_stop_patience': 10,

        # Paths
        'save_path': os.path.join(output_path, 'best_lnn_model.pt'),

        # Task weights
        'task_weights': {
            'location': 0.5,
            'activity': 1.0,
            'count': 0.2,
            'posture': 0.2,
            'kl': 0.1
        }
    }

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )

    # Create model
    print("Creating LNN-based WiFi sensing model...")
    model = LiquidWiFiSensingModel(config)

    # Print model summary
    print("Model Architecture:")
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total Parameters: {total_params:,}")
    print(f"Trainable Parameters: {trainable_params:,}")

    # Train model
    print("Starting training...")
    model, history = train_model(model, train_loader, val_loader, config)

    # Load best model for evaluation
    best_model = LiquidWiFiSensingModel(config)
    checkpoint = torch.load(config['save_path'])
    best_model.load_state_dict(checkpoint['model_state_dict'])

    # Evaluate on test set
    print("Evaluating on test set...")
    # test_loss, test_acc = evaluate_model(best_model, test_loader, config)
    results = enhanced_evaluate_model(best_model, test_loader, config)
    test_loss = results['test_loss']
    test_acc = results['test_acc']

    # Print final results
    print("Training complete!")
    print(f"Best validation loss: {checkpoint['val_loss']:.4f}")
    print(f"Best validation accuracy: {checkpoint['val_acc']:.2f}%")
    print(f"Test loss: {test_loss:.4f}")
    print(f"Test accuracy: {test_acc:.2f}%")
    print(f"Test Accuracy: {results['test_acc']:.2f}% ± {results['test_acc_std']:.2f}%")
    print(f"Precision: {results['precision']:.4f}")
    print(f"Recall: {results['recall']:.4f}")
    print(f"F1 Score: {results['f1']:.4f}")
    print(f"ROC AUC (micro-average): {results['roc_auc']:.4f}")
    # Clean up
    train_dataset.close()
    val_dataset.close()
    test_dataset.close()


if __name__ == "__main__":
    # Check if torchdiffeq is properly installed
    try:
        import torchdiffeq
        print(f"Using torchdiffeq version: {getattr(torchdiffeq, '__version__', 'unknown')}")
    except ImportError:
        print("torchdiffeq is not installed. Please install it using:")
        print("pip install torchdiffeq")
        exit(1)

    # Check PyTorch version for compatibility
    import torch
    pytorch_version = torch.__version__
    print(f"Using PyTorch version: {pytorch_version}")
    major, minor = map(int, pytorch_version.split('.')[:2])
    if major < 1 or (major == 1 and minor < 9):
        print("Warning: This code uses features that require PyTorch 1.9+")
        print("Some functionality may not work with your PyTorch version")

    # Example usage
    data_path = '/content/processed_data'  # Path to processed WiMANS data
    output_path = '/content/lnn_model_output'  # Path to save model and results

    # Call the main function to start the training and evaluation
    main(data_path, output_path)
